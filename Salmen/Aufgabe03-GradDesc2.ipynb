{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3441038a",
   "metadata": {},
   "source": [
    "# Lösung zu Aufgabe 3, Gradientenabstieg2\n",
    "\n",
    "### eine mögliche Lösung von Jan Salmen\n",
    "\n",
    "### Aufgabe und Hintergrund -> siehe Folie 92 (Vorlesung03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f77bbd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9980b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildMiniBatches(n_inputDim, nTrainExamplesPerMiniBatch, nBatch):\n",
    "    miniBatches = []\n",
    "    for b in range(0, nBatch):\n",
    "        miniBatches.append( np.random.uniform( low=-10, high=10, size=(nTrainExamplesPerMiniBatch, n_inputDim) ) )\n",
    "    return miniBatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc119237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isConverged(vecW, n, m):\n",
    "    toRet = True\n",
    "    maxDist = 0.001\n",
    "    for i, wi in enumerate (vecW):\n",
    "        target = 1 if (i < m) else 0\n",
    "        if abs(target - wi) >= maxDist:\n",
    "            toRet = False;\n",
    "            break;\n",
    "    return toRet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce9fefb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainData, n, m, learnRate):\n",
    "    nTrainExamplesConsidered = 0\n",
    "    nMaxEpoches = 10000\n",
    "    \n",
    "    w = np.random.uniform( low=-1, high=2, size=n )  # Gewichtsvektor w zufällig initialisieren. Mittelwert 0,5 \n",
    "    \n",
    "    for epoche in range(0, nMaxEpoches):\n",
    "        for currentBatch in trainData:          \n",
    "            np.random.shuffle(currentBatch)  # Mischen!\n",
    "            g = n_inputDim*[0]               # Zusammengefasster Gradient aus Mini-Batch\n",
    "            for x in currentBatch:\n",
    "\n",
    "                y = 0\n",
    "                for idx in range (0, m):\n",
    "                    y = y + x[idx]        # Zielwert (Summe der ersten m Elemente in x)\n",
    "\n",
    "                yHat = np.inner(x, w)     # Ausgabe unseres Netzes\n",
    "\n",
    "                partD = (yHat - y) * x    #  Partielle Ableitungen nach w_i\n",
    "                g = g + partD\n",
    "\n",
    "            w = w - learnRate * g\n",
    "            nTrainExamplesConsidered = nTrainExamplesConsidered + len(currentBatch)\n",
    "            \n",
    "            if ( isConverged(w, n, m) ):\n",
    "                return nTrainExamplesConsidered\n",
    "    \n",
    "    #print (f\"w not converged: {w}\")\n",
    "    return math.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13221927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeanNumberTrainExamples(miniBatches, n_inputDim, m_inputRelevant, learnRate, nRun):\n",
    "    nTrainExamplesMean = 0\n",
    "    \n",
    "    for idxExp in range ( 0, nRun ):\n",
    "        nTrainExamplesConsidered = train(miniBatches, n_inputDim, m_inputRelevant, learnRate)\n",
    "        nTrainExamplesMean = nTrainExamplesMean + nTrainExamplesConsidered\n",
    "\n",
    "    nTrainExamplesMean = nTrainExamplesMean / nRun\n",
    "    \n",
    "    return nTrainExamplesMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdec3513",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputDim = 20  # n\n",
    "\n",
    "nTrainExamplesPerMiniBatch = 10  # Wie viele Trainingsbeispiele nutzen wir pro Mini-Batch?\n",
    "nBatch = 10  # Wie viele Mini-Batch-Datensätze haben wir insgesamt?\n",
    "\n",
    "miniBatches = buildMiniBatches(n_inputDim, nTrainExamplesPerMiniBatch, nBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e25f03b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: 2 ex: 4445.2\n",
      "m: 4 ex: 4497.4\n",
      "m: 6 ex: 4572.4\n",
      "m: 8 ex: 4367.8\n",
      "m: 10 ex: 4400.0\n",
      "m: 12 ex: 4227.4\n",
      "m: 14 ex: 4307.8\n",
      "m: 16 ex: 4401.0\n",
      "m: 18 ex: 4633.2\n",
      "m: 20 ex: 4512.2\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1 -> Einfluss von m bei festem n, fester Lernrate\n",
    "learnRate = 0.0001  # Lernrate\n",
    "nRun = 50\n",
    "for m_inputRelevant in [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]:\n",
    "    nTrainExamplesMean = getMeanNumberTrainExamples(miniBatches, n_inputDim, m_inputRelevant, learnRate, nRun)\n",
    "    print(f\"m: {m_inputRelevant} ex: {nTrainExamplesMean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e32c2cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learnRate: 1e-05 ex: 56565.5\n",
      "learnRate: 3.162e-05 ex: 17965.0\n",
      "learnRate: 0.0001 ex: 5477.0\n",
      "learnRate: 0.0003162 ex: 1545.0\n",
      "learnRate: 0.001 ex: 658.5\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2 -> Einfluss der Lernrate\n",
    "m_inputRelevant = 10  # m\n",
    "nRun = 50\n",
    "for learnRate in [1e-5, 3.162e-5, 1e-4, 3.162e-4, 1e-3]:\n",
    "    nTrainExamplesMean = getMeanNumberTrainExamples(miniBatches, n_inputDim, m_inputRelevant, learnRate, nRun)\n",
    "    print(f\"learnRate: {learnRate} ex: {nTrainExamplesMean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7104cf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B: 1 ex: 435.02\n",
      "B: 2 ex: 419.6\n",
      "B: 5 ex: 357.4\n",
      "B: 10 ex: 425.0\n",
      "B: 20 ex: 469.6\n",
      "B: 25 ex: 660.0\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3 -> Einfluss der Batch-Größe\n",
    "m_inputRelevant = 10  # m\n",
    "learnRate = 0.001\n",
    "nRun = 50\n",
    "for nTrainExamplesPerMiniBatch in [1, 2, 5, 10, 20, 25]:\n",
    "    nBatch = 100 // nTrainExamplesPerMiniBatch  # Ganzzahlige Division.  Wir tun so, als hätten wir immer 100 Trainingsbeispiele, die wir unterschiedliche aufteilen\n",
    "    miniBatches = buildMiniBatches(n_inputDim, nTrainExamplesPerMiniBatch, nBatch)\n",
    "    nTrainExamplesMean = getMeanNumberTrainExamples(miniBatches, n_inputDim, m_inputRelevant, learnRate, nRun)\n",
    "    print(f\"B: {nTrainExamplesPerMiniBatch} ex: {nTrainExamplesMean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c07370b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
