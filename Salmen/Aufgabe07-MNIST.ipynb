{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quelle: https://www.kaggle.com/code/enwei26/mnist-digits-pytorch-cnn-99/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27141,
     "status": "ok",
     "timestamp": 1651525241391,
     "user": {
      "displayName": "Jan S.",
      "userId": "11619947665711584083"
     },
     "user_tz": -120
    },
    "id": "lTUoB9FDRaRh",
    "outputId": "24805615-60e4-4fe5-d8a4-93f8f4e05064"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Daten zum Download:\n",
    "# https://www.kaggle.com/c/digit-recognizer\n",
    "# https://www.kaggle.com/code/jagdish2386/fashion-mnist-dnn/data\n",
    "\n",
    "# Das hier f√ºr Google Colab:\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#%cd /content/drive/My\\ Drive/Colab\\ Notebooks\n",
    "\n",
    "import os\n",
    "filesInput = os.listdir(\"./input\")\n",
    "print( f\"files found in input dir: {filesInput}\" )\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print( f\"device: {device}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7WX16VcRaRm"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6316,
     "status": "ok",
     "timestamp": 1651525251712,
     "user": {
      "displayName": "Jan S.",
      "userId": "11619947665711584083"
     },
     "user_tz": -120
    },
    "id": "u_L-2xHYRaRo",
    "outputId": "6e93b4bd-7307-4427-c9d2-2cc67c845ef9"
   },
   "outputs": [],
   "source": [
    "# MNIST\n",
    "#df_train = pd.read_csv('./input/train.csv')\n",
    "#df_test = pd.read_csv('./input/test.csv')\n",
    "\n",
    "# Fashion MNIST\n",
    "df_train = pd.read_csv('./input/fashion-mnist_train.csv')\n",
    "df_test = pd.read_csv('./input/fashion-mnist_test.csv')\n",
    "\n",
    "print('Train size: ', df_train.shape)\n",
    "print('Test size: ', df_test.shape)\n",
    "#df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uw2JwfuaRaRq"
   },
   "source": [
    "### Calculate mean and std of training data - used for normalization later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 514,
     "status": "ok",
     "timestamp": 1651525305098,
     "user": {
      "displayName": "Jan S.",
      "userId": "11619947665711584083"
     },
     "user_tz": -120
    },
    "id": "z_2Z10tQRaRq",
    "outputId": "b6f3b5cc-509d-442b-a79c-97725fb482b2"
   },
   "outputs": [],
   "source": [
    "train_data = df_train.drop('label', axis=1).values\n",
    "train_mean = train_data.mean()/255.\n",
    "train_std = train_data.std()/255.\n",
    "print( f\"Mean: {train_mean}  StdDev: {train_std}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loUhXFJiRaRr"
   },
   "source": [
    "### Split training data into training-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1651525308771,
     "user": {
      "displayName": "Jan S.",
      "userId": "11619947665711584083"
     },
     "user_tz": -120
    },
    "id": "cfiElRxlRaRr",
    "outputId": "4d397856-2225-47af-b91f-ab723f965e41"
   },
   "outputs": [],
   "source": [
    "# Train-Val split\n",
    "mask = np.random.rand(len(df_train)) < 0.8\n",
    "df_val = df_train[~mask]\n",
    "df_train = df_train[mask]\n",
    "print('Train size: ', df_train.shape)\n",
    "print('Val size: ', df_val.shape)\n",
    "print('Test size: ', df_test.shape)\n",
    "#df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQa-COR9RaRs"
   },
   "source": [
    "### Visualize Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1651507444062,
     "user": {
      "displayName": "Jan S.",
      "userId": "11619947665711584083"
     },
     "user_tz": -120
    },
    "id": "EZr8fQTGRaRs",
    "outputId": "f70cea78-3238-423a-a184-87321fb47f47"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ind = np.random.randint(0, df_train.shape[0]-1)\n",
    "plt.imshow(df_train.iloc[ind].values[1:].reshape((28,28)), cmap='gray')\n",
    "plt.title(str(df_train.iloc[ind][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1exaIdnRaRu"
   },
   "source": [
    "### Define a PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1651525316762,
     "user": {
      "displayName": "Jan S.",
      "userId": "11619947665711584083"
     },
     "user_tz": -120
    },
    "id": "AOglk0QARaRu"
   },
   "outputs": [],
   "source": [
    "# Create dataset class for PyTorch\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        data = self.df.iloc[n]\n",
    "        image = data[1:].values.reshape((28,28)).astype(np.uint8)\n",
    "        label = data[0]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWUXca-cRaRv"
   },
   "source": [
    "### Define data augmentation and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1651529627094,
     "user": {
      "displayName": "Jan S.",
      "userId": "11619947665711584083"
     },
     "user_tz": -120
    },
    "id": "R_bVLp3lRaRw"
   },
   "outputs": [],
   "source": [
    "# Initialize transformation, datasets, and loaders\n",
    "batch_size = 32\n",
    "classes = range(10)\n",
    "train_transform = transforms.Compose(\n",
    "                    [\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.RandomAffine(degrees=(-10,10), translate=(0.1,0.1), scale=(0.95, 1.05)),\n",
    "                    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[train_mean], std=[train_std]),\n",
    "                    ])\n",
    "# don't (really) need the data augmentation in validation\n",
    "val_transform = transforms.Compose(\n",
    "                    [\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[train_mean], std=[train_std]),\n",
    "                    ])\n",
    "test_transform = val_transform\n",
    "\n",
    "train_dataset = MNISTDataset(df_train, transform = train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = MNISTDataset(df_val, transform = val_transform)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIP1wSMzRaRx"
   },
   "source": [
    "### Sanity check to make sure data is of normal distribution (zero mean and unit standard dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1651529648036,
     "user": {
      "displayName": "Jan S.",
      "userId": "11619947665711584083"
     },
     "user_tz": -120
    },
    "id": "9APGiiNmRaRy",
    "outputId": "fc92aa1e-ed79-4dd4-9277-b959c2d68bc2"
   },
   "outputs": [],
   "source": [
    "# sanity check for training data\n",
    "imgs, lbls = next(iter(train_loader))\n",
    "imgs[7].data.shape\n",
    "print(imgs.data.min())\n",
    "print(imgs.data.max())\n",
    "print(imgs.data.mean())\n",
    "print(imgs.data.std())\n",
    "print(classes[lbls[0]])\n",
    "plt.imshow(imgs[0].data.reshape((28,28)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hc3S5NAyRaRy"
   },
   "outputs": [],
   "source": [
    "# sanity check for validation data\n",
    "imgs, lbls = next(iter(val_loader))\n",
    "imgs[0].data.shape\n",
    "print(imgs.data.min())\n",
    "print(imgs.data.max())\n",
    "print(imgs.data.mean())\n",
    "print(imgs.data.std())\n",
    "print(classes[lbls[0]])\n",
    "plt.imshow(imgs[0].data.reshape((28,28)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NOuUE5JRaRz"
   },
   "source": [
    "### Define network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1651529657673,
     "user": {
      "displayName": "Jan S.",
      "userId": "11619947665711584083"
     },
     "user_tz": -120
    },
    "id": "Pe36f0XYRaR0"
   },
   "outputs": [],
   "source": [
    "# model definition\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 100),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(100, 10)\n",
    "        )\n",
    "                \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.hidden1(x)     \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ozInJswRaR1"
   },
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1651529670556,
     "user": {
      "displayName": "Jan S.",
      "userId": "11619947665711584083"
     },
     "user_tz": -120
    },
    "id": "3eoL4bm5RaR2"
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "model = Model()\n",
    "model.to(device)\n",
    "criterion = nn.NLLLoss()   # with log_softmax() as the last layer, this is equivalent to cross entropy loss\n",
    "optimizer = SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4ahm1gBRaR3",
    "outputId": "19814a5b-4815-4aa8-e778-e2303f16e11d"
   },
   "outputs": [],
   "source": [
    "# Training Time!\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Some initialization work first...\n",
    "epochs = 500\n",
    "train_losses, val_losses = [], []\n",
    "train_accu, val_accu = [], []\n",
    "start_time = time.time()\n",
    "early_stop_counter = 10   # stop when the validation loss does not improve for 10 iterations to prevent overfitting\n",
    "counter = 0\n",
    "best_val_loss = float('Inf')\n",
    "\n",
    "for e in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    running_loss = 0\n",
    "    accuracy=0\n",
    "    # training step\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        log_ps = model(images)\n",
    "        \n",
    "        ps = torch.exp(log_ps)                \n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        \n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # record training loss and error, then evaluate using validation data\n",
    "    train_losses.append(running_loss/len(train_loader))\n",
    "    train_accu.append(accuracy/len(train_loader))\n",
    "    val_loss = 0\n",
    "    accuracy=0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            log_ps = model(images)\n",
    "            val_loss += criterion(log_ps, labels)\n",
    "\n",
    "            ps = torch.exp(log_ps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "    val_losses.append(val_loss/len(val_loader))\n",
    "    val_accu.append(accuracy/len(val_loader))\n",
    "\n",
    "    print(\"Epoch: {}/{} \".format(e+1, epochs),\n",
    "          \"Time: {:.2f}s \".format(time.time()-epoch_start_time),\n",
    "          \"Training Loss: {:.3f} \".format(train_losses[-1]),\n",
    "          \"Training Accu: {:.2f}% \".format(100*train_accu[-1]),\n",
    "          \"Val Loss: {:.3f} \".format(val_losses[-1]),\n",
    "          \"Val Accu: {:.2f}%\".format(100*val_accu[-1]))\n",
    "\n",
    "    print( '  Epoch took %6.2fs, toal %8.2fs' % (time.time()-epoch_start_time, time.time()-start_time) )\n",
    "    \n",
    "    if val_losses[-1] < best_val_loss:\n",
    "        best_val_loss = val_losses[-1]\n",
    "        counter=0\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    else:\n",
    "        counter+=1\n",
    "        print('Validation loss has not improved since: {:.3f}..'.format(best_val_loss), 'Count: ', str(counter))\n",
    "        if counter >= early_stop_counter:\n",
    "            print('Early Stopping now!!')\n",
    "            model.load_state_dict(best_model_wts)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "executionInfo": {
     "elapsed": 913,
     "status": "ok",
     "timestamp": 1651529525210,
     "user": {
      "displayName": "Jan S.",
      "userId": "11619947665711584083"
     },
     "user_tz": -120
    },
    "id": "5EjbVwoJRaR5",
    "outputId": "f20cb380-8aba-4540-c8d0-ad2091c6ec61"
   },
   "outputs": [],
   "source": [
    "# plot training history\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(2,1,1)\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, e + 2])\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(range(1, e + 2), torch.FloatTensor(train_losses[:e+1]), 'b', label='Training Loss')\n",
    "plt.plot(range(1, e + 2), torch.FloatTensor(val_losses[:e+1]), 'r', label='Validation Loss')\n",
    "ax.grid(linestyle='-.')\n",
    "plt.legend()\n",
    "plt.subplot(2,1,2)\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, e+2])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(range(1, e + 2), torch.FloatTensor(train_accu[:e+1]), 'b', label='Training Accuracy')\n",
    "plt.plot(range(1, e + 2), torch.FloatTensor(val_accu[:e+1]), 'r', label='Validation Accuracy')\n",
    "ax.grid(linestyle='-.')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nw1WH-EJRaR6"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oka2UVrCRaR6"
   },
   "outputs": [],
   "source": [
    "# prepare to predict test data - REMEMBER PRE-PROCESSING!\n",
    "# I originally forgot to scale and normalize, which caused problems....\n",
    "\n",
    "# some sanity check to make sure\n",
    "x_test = df_test.values\n",
    "x_test = x_test.reshape([-1, 28, 28]).astype(float)\n",
    "x_test = x_test/255.\n",
    "x_test = (x_test-train_mean)/train_std\n",
    "print(x_test.min())\n",
    "print(x_test.max())\n",
    "print(x_test.mean())\n",
    "print(x_test.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0smAK9MDRaR7"
   },
   "outputs": [],
   "source": [
    "x_test = np.expand_dims(x_test, axis=1)\n",
    "x_test = torch.from_numpy(x_test).float().to(device)\n",
    "x_test.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2dUxuQYRaR7"
   },
   "outputs": [],
   "source": [
    "# prediction time!\n",
    "model.eval()   # this is needed to disable dropouts\n",
    "with torch.no_grad():    # turn off gradient computation because we don't need it for prediction\n",
    "    ps = model(x_test)\n",
    "    prediction = torch.argmax(ps, 1)\n",
    "    print('Prediction', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_M3qHhJRaR8"
   },
   "outputs": [],
   "source": [
    "# prepare output file\n",
    "df_export = pd.DataFrame(prediction.cpu().tolist(), columns = ['Label'])\n",
    "df_export['ImageId'] = df_export.index +1\n",
    "df_export = df_export[['ImageId', 'Label']]\n",
    "df_export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7Q8fVvYRaR8"
   },
   "outputs": [],
   "source": [
    "df_export.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mnist-digits-pytorch-cnn-99.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
